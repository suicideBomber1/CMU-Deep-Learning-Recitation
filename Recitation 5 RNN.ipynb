{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data downloaded from https://www.kaggle.com/kingburrito666/better-donald-trump-tweets/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_length = 3\n",
    "\n",
    "def process_input(filename):\n",
    "    text = open(filename).read()\n",
    "    sequences = [text[i:i+history_length] for i in range(int(len(text)/history_length))]\n",
    "    stats = Counter(sequences)\n",
    "    tokens = []\n",
    "    counts = []\n",
    "    for i in stats.most_common():\n",
    "        tokens.append(i[0])\n",
    "        counts.append(i[1])\n",
    "    return stats\n",
    "\n",
    "def next_char(cur, stats):\n",
    "    seed = cur[1:]\n",
    "    candidates = []\n",
    "    candidatec = []\n",
    "    for k in stats.keys():\n",
    "        if seed == k[:-1]:\n",
    "            candidates.append(k)\n",
    "            candidatec.append(stats[k])\n",
    "    candidatep = [x/sum(candidatec) for x in candidatec]\n",
    "    return candidates[np.random.choice(len(candidates), p=candidatep)]\n",
    "\n",
    "def sample(length, running_state, stats):\n",
    "    output = ''\n",
    "    for i in range(length):\n",
    "        output += running_state[0]\n",
    "        running_state = next_char(running_state, stats)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'obamil sh  dibizonlyised sed new govemit co zvknow'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_stats = process_input('tweets.txt')\n",
    "sample(50, 'oba', tweets_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'obamazing scrimes  afters any of the   ver  ters b'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_length = 4\n",
    "\n",
    "tweets_stats = process_input('tweets.txt')\n",
    "sample(50, 'obam', tweets_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and to say we end the mind that fles, and that fle'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet_stats = process_input('hamlet.txt')\n",
    "sample(50, 'and ', hamlet_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Above method just uses the history upto a certain number of characters to match the characters. But, there is no context of language generation, eventhough we increase the history length and also it is more memorization than text generation. \n",
    "> RNN's shine better in such cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAD8CAYAAACPWyg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1133d9390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "T = 80 # range from which ints are sampled\n",
    "L = 1000 # Length of the sequence\n",
    "N = 100 # Number of examples\n",
    "\n",
    "future = 1000 # Length of the sequence to predict\n",
    "\n",
    "# Generating time series data\n",
    "x = np.empty((N, L), 'int64')\n",
    "x[:] = np.array(range(L)) + np.random.randint(-4*T, 4*T, N).reshape(N, 1)\n",
    "data = np.sin(x/1.0/T).astype('float64')\n",
    "\n",
    "# Visulizing the input data\n",
    "plt.plot(x, data, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequence, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1, 32)\n",
    "        self.lstm2 = nn.LSTMCell(32, 32)\n",
    "        self.linear = nn.Linear(32, 1)\n",
    "    def forward(self, input, future=0):\n",
    "        outputs = []\n",
    "        h_t = Variable(torch.zeros(input.size(0), 32).double(), requires_grad=False)\n",
    "        c_t = Variable(torch.zeros(input.size(0), 32).double(), requires_grad=False)\n",
    "        h_t2 = Variable(torch.zeros(input.size(0), 32).double(), requires_grad=False)\n",
    "        c_t2 = Variable(torch.zeros(input.size(0), 32).double(), requires_grad=False)\n",
    "        \n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        for i in range(future):# if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs\n",
    "    \n",
    "def save_plot_wave(y_gen):\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    plt.title('Predict future values for time sequence', fontsize=30)\n",
    "    plt.xlabel('x', fontsize=20)\n",
    "    plt.ylabel('y', fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.plot(np.arange(input.size(1)), y_gen[0][:input.size(1)], 'b', linewidth=2.0)\n",
    "    plt.plot(np.arange(input.size(1), input.size(1)+future), y_gen[0][input.size(1):], 'b'+':', linewidth=2.0)\n",
    "    plt.savefig('predicted%d.pdf'%i)\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.from_numpy(data[1:, :-1]), requires_grad=False)\n",
    "target = Variable(torch.from_numpy(data[1:, 1:]), requires_grad=False)\n",
    "test_input = Variable(torch.from_numpy(data[:1, :-1]), requires_grad=False)\n",
    "test_target = Variable(torch.from_numpy(data[:1, 1:]), requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "Loss: 0.516380324626\n",
      "Loss: 0.497592573371\n",
      "Loss: 0.470569010796\n",
      "Loss: 0.456216711725\n",
      "Loss: 0.393666559957\n",
      "Loss: 1.65155456462\n",
      "Loss: 0.278070944432\n",
      "Loss: 0.177961647481\n",
      "Loss: 3.36771876436\n",
      "Loss: 0.0299007703389\n",
      "Loss: 0.0108930218262\n",
      "Loss: 0.00759165235046\n",
      "Loss: 0.00722119699241\n",
      "Loss: 0.00693554349076\n",
      "Loss: 0.00636568045592\n",
      "Loss: 0.0055249328318\n",
      "Loss: 0.00449744715725\n",
      "Loss: 0.0037286760957\n",
      "Loss: 0.00356010611745\n",
      "Loss: 0.00352051438588\n",
      "Test loss: 0.00309448016137\n",
      "Step:  1\n",
      "Loss: 0.00346174323829\n",
      "Loss: 0.00330873190325\n",
      "Loss: 0.00294094703106\n",
      "Loss: 0.00209040120307\n",
      "Loss: 0.00135959128856\n",
      "Loss: 0.00106475542988\n",
      "Loss: 0.000960154317636\n",
      "Loss: 0.000923589575787\n",
      "Loss: 0.000901479715146\n",
      "Loss: 0.000887053423266\n",
      "Loss: 0.000863068552769\n",
      "Loss: 0.000797854431623\n",
      "Loss: 0.000657260841894\n",
      "Loss: 0.00314928170681\n",
      "Loss: 0.000544998709543\n",
      "Loss: 0.000529940174493\n",
      "Loss: 0.000483158670381\n",
      "Loss: 0.000434186480961\n",
      "Loss: 0.000430319375456\n",
      "Loss: 0.00041734653318\n",
      "Test loss: 0.000370317587974\n",
      "Step:  2\n",
      "Loss: 0.000414776536912\n",
      "Loss: 0.000395217890129\n",
      "Loss: 0.000383935191792\n",
      "Loss: 0.000369627643007\n",
      "Loss: 0.000350905743637\n",
      "Loss: 0.000333226757175\n",
      "Loss: 0.000319750079553\n",
      "Loss: 0.000312487238171\n",
      "Loss: 0.00030545712346\n",
      "Loss: 0.00029579500772\n",
      "Loss: 0.000267058446159\n",
      "Loss: 0.000216278678754\n",
      "Loss: 0.000210463769709\n",
      "Loss: 0.000195643902742\n",
      "Loss: 0.000175507566332\n",
      "Loss: 0.000172617599382\n",
      "Loss: 0.000154617883221\n",
      "Loss: 0.000145649626385\n",
      "Loss: 0.000140301946069\n",
      "Loss: 0.00013642532787\n",
      "Test loss: 0.000104150984465\n",
      "Step:  3\n",
      "Loss: 0.0001248754801\n",
      "Loss: 9.69976108392e-05\n",
      "Loss: 7.65998387662e-05\n",
      "Loss: 7.25769007421e-05\n",
      "Loss: 6.60842899768e-05\n",
      "Loss: 6.31319715741e-05\n",
      "Loss: 6.24919504268e-05\n",
      "Loss: 6.19437820422e-05\n",
      "Loss: 6.06678308301e-05\n",
      "Loss: 5.918894465e-05\n",
      "Loss: 5.8018973216e-05\n",
      "Loss: 5.75371227876e-05\n",
      "Loss: 5.72405616542e-05\n",
      "Loss: 5.70448465567e-05\n",
      "Loss: 5.6889145282e-05\n",
      "Loss: 5.68312347798e-05\n",
      "Loss: 5.67682358985e-05\n",
      "Loss: 5.66748551289e-05\n",
      "Loss: 5.65759631212e-05\n",
      "Loss: 5.64669902862e-05\n",
      "Test loss: 4.04180054394e-05\n",
      "Step:  4\n",
      "Loss: 5.63482074137e-05\n",
      "Loss: 5.62081574555e-05\n",
      "Loss: 5.59384682798e-05\n",
      "Loss: 5.50626186832e-05\n",
      "Loss: 5.28710951404e-05\n",
      "Loss: 4.96443435084e-05\n",
      "Loss: 4.63762107707e-05\n",
      "Loss: 5.00447976572e-05\n",
      "Loss: 4.58705258533e-05\n",
      "Loss: 4.18211804417e-05\n",
      "Loss: 3.93778558606e-05\n",
      "Loss: 3.82996458417e-05\n",
      "Loss: 3.61829305267e-05\n",
      "Loss: 3.48635977904e-05\n",
      "Loss: 3.38727009688e-05\n",
      "Loss: 3.33881136804e-05\n",
      "Loss: 3.31540884771e-05\n",
      "Loss: 3.28717174441e-05\n",
      "Loss: 3.28005857625e-05\n",
      "Loss: 3.27173310177e-05\n",
      "Test loss: 2.71113645186e-05\n",
      "Step:  5\n",
      "Loss: 3.24069007713e-05\n",
      "Loss: 3.2029349761e-05\n",
      "Loss: 3.12756231742e-05\n",
      "Loss: 3.0172187393e-05\n",
      "Loss: 2.96781384098e-05\n",
      "Loss: 2.92874381846e-05\n",
      "Loss: 2.97494732925e-05\n",
      "Loss: 2.88828216257e-05\n",
      "Loss: 2.86296612477e-05\n",
      "Loss: 2.84948779231e-05\n",
      "Loss: 2.7973164802e-05\n",
      "Loss: 2.77794367675e-05\n",
      "Loss: 2.74346338856e-05\n",
      "Loss: 2.72596877641e-05\n",
      "Loss: 2.70115003378e-05\n",
      "Loss: 2.68043557498e-05\n",
      "Loss: 2.66570733974e-05\n",
      "Loss: 2.65436734527e-05\n",
      "Loss: 2.63439681915e-05\n",
      "Loss: 2.60051427453e-05\n",
      "Test loss: 1.726209752e-05\n",
      "Step:  6\n",
      "Loss: 2.54501152072e-05\n",
      "Loss: 2.46226227233e-05\n",
      "Loss: 2.41647510979e-05\n",
      "Loss: 2.40926382609e-05\n",
      "Loss: 2.40662740919e-05\n",
      "Loss: 2.40538373945e-05\n",
      "Loss: 2.40401313581e-05\n",
      "Loss: 2.39994892287e-05\n",
      "Loss: 2.39904744739e-05\n",
      "Loss: 2.39807820036e-05\n",
      "Loss: 2.39674452778e-05\n",
      "Loss: 2.3923073336e-05\n",
      "Loss: 2.38355409512e-05\n",
      "Loss: 2.36493230782e-05\n",
      "Loss: 2.32172948205e-05\n",
      "Loss: 2.22592680195e-05\n",
      "Loss: 2.14751632745e-05\n",
      "Loss: 2.62012942329e-05\n",
      "Loss: 1.99549220913e-05\n",
      "Loss: 1.87108023013e-05\n",
      "Test loss: 3.47789793681e-05\n",
      "Step:  7\n",
      "Loss: 3.79363983855e-05\n",
      "Loss: 1.69414669229e-05\n",
      "Loss: 1.6042422652e-05\n",
      "Loss: 1.9152017915e-05\n",
      "Loss: 1.42337680855e-05\n",
      "Loss: 1.37506250411e-05\n",
      "Loss: 1.27288497629e-05\n",
      "Loss: 1.189093227e-05\n",
      "Loss: 1.16263800604e-05\n",
      "Loss: 1.13068549717e-05\n",
      "Loss: 1.0987549036e-05\n",
      "Loss: 1.06441516015e-05\n",
      "Loss: 1.014034363e-05\n",
      "Loss: 9.53483317885e-06\n",
      "Loss: 9.06801139205e-06\n",
      "Loss: 7.04428000614e-06\n",
      "Loss: 6.4318431029e-06\n",
      "Loss: 6.24077018213e-06\n",
      "Loss: 6.07850913612e-06\n",
      "Loss: 5.91441963467e-06\n",
      "Test loss: 6.39309011457e-06\n",
      "Step:  8\n",
      "Loss: 5.82859026216e-06\n",
      "Loss: 5.75623674699e-06\n",
      "Loss: 5.68247059008e-06\n",
      "Loss: 5.60384355449e-06\n",
      "Loss: 5.55837791626e-06\n",
      "Loss: 5.46553390326e-06\n",
      "Loss: 5.22235350753e-06\n",
      "Loss: 4.93019728548e-06\n",
      "Loss: 4.59638634295e-06\n",
      "Loss: 4.06241856152e-06\n",
      "Loss: 4.16218525664e-06\n",
      "Loss: 3.94085817211e-06\n",
      "Loss: 3.67176504194e-06\n",
      "Loss: 3.56515450044e-06\n",
      "Loss: 3.50926162808e-06\n",
      "Loss: 3.45959618758e-06\n",
      "Loss: 3.39285148061e-06\n",
      "Loss: 3.34702603337e-06\n",
      "Loss: 3.32259685737e-06\n",
      "Loss: 3.3104453767e-06\n",
      "Test loss: 2.18737734898e-06\n",
      "Step:  9\n",
      "Loss: 3.30740183516e-06\n",
      "Loss: 3.30232266913e-06\n",
      "Loss: 3.29830133533e-06\n",
      "Loss: 3.28737769784e-06\n",
      "Loss: 3.2691159922e-06\n",
      "Loss: 3.24177568101e-06\n",
      "Loss: 3.21387911724e-06\n",
      "Loss: 3.19683637346e-06\n",
      "Loss: 3.18590377157e-06\n",
      "Loss: 3.17587611506e-06\n",
      "Loss: 3.17206353803e-06\n",
      "Loss: 3.16997876729e-06\n",
      "Loss: 3.15638877679e-06\n",
      "Loss: 3.04319629768e-06\n",
      "Loss: 2.93330735453e-06\n",
      "Loss: 2.8375729312e-06\n",
      "Loss: 2.90714630609e-06\n",
      "Loss: 2.69604099842e-06\n",
      "Loss: 2.64340369809e-06\n",
      "Loss: 2.56688608503e-06\n",
      "Test loss: 1.829920588e-06\n",
      "Step:  10\n",
      "Loss: 2.53450968599e-06\n",
      "Loss: 2.51067102462e-06\n",
      "Loss: 2.48911540223e-06\n",
      "Loss: 2.47067299241e-06\n",
      "Loss: 2.45343886939e-06\n",
      "Loss: 2.44212505632e-06\n",
      "Loss: 2.43613504398e-06\n",
      "Loss: 2.43133778461e-06\n",
      "Loss: 2.42714571044e-06\n",
      "Loss: 2.42105921054e-06\n",
      "Loss: 2.41231459264e-06\n",
      "Loss: 2.40320292743e-06\n",
      "Loss: 2.39464364801e-06\n",
      "Loss: 2.38524732968e-06\n",
      "Loss: 2.36429997656e-06\n",
      "Loss: 2.32414208886e-06\n",
      "Loss: 2.26041119543e-06\n",
      "Loss: 2.16397186806e-06\n",
      "Loss: 2.04628265882e-06\n",
      "Loss: 2.08660208472e-06\n",
      "Test loss: 1.41649601853e-06\n"
     ]
    }
   ],
   "source": [
    "seq = Sequence()\n",
    "seq.double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n",
    "#begin to train\n",
    "for i in range(11):\n",
    "    print('Step: ', i)\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = seq(input)\n",
    "        loss = criterion(out, target)\n",
    "        print('Loss:', loss.data.numpy()[0])\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(closure)\n",
    "    # begin to predict\n",
    "    pred = seq(test_input, future = future)\n",
    "    loss = criterion(pred[:, :-future], test_target)\n",
    "    print('Test loss:', loss.data.numpy()[0])\n",
    "    y = pred.data.numpy()\n",
    "    save_plot_wave(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
